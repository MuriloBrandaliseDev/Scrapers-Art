# Sistema de Web Scraping para LeilÃµes de Arte

Sistema profissional e modular para coleta de dados de quadros e esculturas de sites de leilÃµes online.

## ğŸ¯ Sites Suportados

- âœ… **iArremate** - Belas Artes (Quadros)
- âœ… **LeilÃµesBR** - Quadros e Esculturas

## ğŸ“‹ CaracterÃ­sticas

- âœ… **Arquitetura Modular**: Sistema base extensÃ­vel para novos scrapers
- âœ… **API REST FastAPI**: Controle via endpoints HTTP
- âœ… **MÃºltiplas EstratÃ©gias**: ExtraÃ§Ã£o robusta com fallbacks
- âœ… **Suporte a Redirecionamentos**: Detecta e processa sites redirecionados
- âœ… **Logging Profissional**: Sistema completo de logs
- âœ… **Retry AutomÃ¡tico**: Tratamento robusto de erros
- âœ… **Headers AleatÃ³rios**: Evita bloqueios
- âœ… **Pronto para ProduÃ§Ã£o**: Estrutura organizada para servidores

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)

### Passos

1. **Clone ou baixe o repositÃ³rio**

2. **Crie um ambiente virtual (recomendado):**
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

3. **Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

## ğŸ“– Uso

### OpÃ§Ã£o 1: Scripts Diretos

#### iArremate
```bash
python run.py
```

#### LeilÃµesBR
```bash
python run_leiloes_br.py
```

### OpÃ§Ã£o 2: API FastAPI (Recomendado para Servidores)

#### Iniciar a API
```bash
python start_api.py
```

A API estarÃ¡ disponÃ­vel em: `http://localhost:8000`

#### DocumentaÃ§Ã£o Interativa
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

#### Endpoints DisponÃ­veis

**Iniciar Scraper iArremate:**
```bash
curl -X POST "http://localhost:8000/api/v1/iarremate" \
  -H "Content-Type: application/json" \
  -d '{"max_paginas": 10, "delay_between_requests": 1.0}'
```

**Iniciar Scraper LeilÃµesBR:**
```bash
curl -X POST "http://localhost:8000/api/v1/leiloes-br" \
  -H "Content-Type: application/json" \
  -d '{
    "categorias": ["quadros", "esculturas"],
    "max_paginas": 10,
    "delay_between_requests": 1.0
  }'
```

**Verificar Status:**
```bash
curl "http://localhost:8000/api/v1/status/{scraper_id}"
```

**Listar Todos os Scrapers:**
```bash
curl "http://localhost:8000/api/v1/scrapers"
```

### OpÃ§Ã£o 3: Como MÃ³dulo Python

```python
from src.iarremate_scraper import IArremateScraper
from src.leiloes_br_scraper import LeiloesBRScraper

# iArremate
scraper_iarremate = IArremateScraper()
scraper_iarremate.executar_scraping(max_paginas=10)
scraper_iarremate.salvar_planilha()

# LeilÃµesBR
scraper_leiloes = LeiloesBRScraper()
scraper_leiloes.executar_scraping(
    categorias=["quadros", "esculturas"],
    max_paginas=10
)
scraper_leiloes.salvar_planilha()
```

## ğŸ“Š Dados Coletados

### iArremate
- Nome_Artista
- Categoria
- Pagina
- Titulo
- Descricao
- Valor
- URL
- Data_Coleta

### LeilÃµesBR
- Nome_Artista
- Categoria (Quadros/Esculturas)
- Pagina
- Lote
- Titulo
- Descricao
- Valor
- Data_Leilao
- Leiloeiro
- Local
- URL
- URL_Original
- Site_Redirecionado (quando aplicÃ¡vel)
- Data_Coleta

## ğŸ“ Estrutura do Projeto

```
DesafioWebscrapping/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py          # Classe base abstrata
â”‚   â”œâ”€â”€ iarremate_scraper.py     # Scraper iArremate
â”‚   â””â”€â”€ leiloes_br_scraper.py    # Scraper LeilÃµesBR
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                  # API FastAPI
â”œâ”€â”€ output/                      # Arquivos de saÃ­da (Excel/CSV)
â”œâ”€â”€ logs/                        # Arquivos de log
â”œâ”€â”€ config/                      # Arquivos de configuraÃ§Ã£o
â”œâ”€â”€ run.py                       # Script iArremate
â”œâ”€â”€ run_leiloes_br.py            # Script LeilÃµesBR
â”œâ”€â”€ start_api.py                 # Script para iniciar API
â”œâ”€â”€ requirements.txt             # DependÃªncias
â”œâ”€â”€ .gitignore                   # Arquivos ignorados
â””â”€â”€ README.md                    # Este arquivo
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### ParÃ¢metros do Scraper

```python
scraper = LeiloesBRScraper(
    base_url="https://www.leiloesbr.com.br",
    output_dir="output",              # DiretÃ³rio de saÃ­da
    logs_dir="logs",                  # DiretÃ³rio de logs
    max_retries=3,                    # Tentativas por requisiÃ§Ã£o
    delay_between_requests=1.0        # Delay entre requisiÃ§Ãµes (segundos)
)
```

### Executar com Limites

```python
# Limitar nÃºmero de pÃ¡ginas
scraper.executar_scraping(max_paginas=5)

# Filtrar categorias (LeilÃµesBR)
scraper.executar_scraping(categorias=["quadros"])
```

## ğŸŒ Deploy em Servidor

### Usando uvicorn diretamente:

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

### Usando systemd (Linux):

Crie um arquivo `/etc/systemd/system/scrapers-api.service`:

```ini
[Unit]
Description=Web Scrapers API
After=network.target

[Service]
Type=simple
User=seu_usuario
WorkingDirectory=/caminho/para/projeto
Environment="PATH=/caminho/para/venv/bin"
ExecStart=/caminho/para/venv/bin/uvicorn api.main:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target
```

Ative o serviÃ§o:
```bash
sudo systemctl enable scrapers-api
sudo systemctl start scrapers-api
```

### Usando Docker (opcional):

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ” EstratÃ©gias de ExtraÃ§Ã£o

Cada scraper utiliza mÃºltiplas estratÃ©gias para garantir coleta de dados:

1. **Busca por Texto EspecÃ­fico**: "Valor Atual", "Lance Atual", etc.
2. **Busca por PadrÃµes Regex**: R$ seguido de nÃºmeros
3. **Busca em Classes CSS**: Elementos com classes relacionadas
4. **Busca em Elementos HTML**: Spans, divs, inputs, etc.

## ğŸ“ Logging

O sistema gera logs detalhados em:
- **Console**: SaÃ­da em tempo real
- **Arquivo**: `logs/{scraper_name}_YYYYMMDD_HHMMSS.log`

NÃ­veis de log:
- `INFO`: InformaÃ§Ãµes gerais
- `WARNING`: Avisos (retries, etc.)
- `ERROR`: Erros nÃ£o crÃ­ticos
- `DEBUG`: InformaÃ§Ãµes detalhadas

## âš ï¸ ConsideraÃ§Ãµes Importantes

1. **Respeite os termos de uso dos sites**
2. **Use delays adequados** entre requisiÃ§Ãµes
3. **Os sites podem alterar estrutura** - os scrapers tÃªm fallbacks
4. **Algumas obras podem nÃ£o ter valor disponÃ­vel** - aparecerÃ¡ como "N/A"
5. **LeilÃµesBR pode redirecionar** para outros sites - o sistema detecta automaticamente

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de ConexÃ£o
- Verifique conexÃ£o com internet
- Site pode estar temporariamente indisponÃ­vel
- Verifique logs em `logs/`

### Nenhuma Obra Encontrada
- Site pode ter alterado estrutura
- Verifique URLs base
- Consulte logs para detalhes

### Valores NÃ£o Coletados
- Site pode ter mudado formato de exibiÃ§Ã£o
- Scraper tenta mÃºltiplas estratÃ©gias automaticamente
- Verifique logs para estratÃ©gias utilizadas

### Problemas com Redirecionamentos
- Sistema detecta automaticamente
- Verifique campo "Site_Redirecionado" nos dados
- Logs mostram redirecionamentos

## ğŸ”„ PrÃ³ximas Melhorias

- [ ] Suporte a mais sites de leilÃµes
- [ ] Interface web para monitoramento
- [ ] Suporte a banco de dados
- [ ] Agendamento automÃ¡tico (cron jobs)
- [ ] NotificaÃ§Ãµes por email/webhook
- [ ] Cache de requisiÃ§Ãµes
- [ ] Rate limiting configurÃ¡vel

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido para fins educacionais e profissionais.

## ğŸ¤ ContribuiÃ§Ã£o

SugestÃµes e melhorias sÃ£o bem-vindas!

---

**Desenvolvido por MuriloDEV** ğŸš€
