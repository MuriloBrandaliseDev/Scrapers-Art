#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Web Scraper para Leil√µesBR - Quadros e Esculturas
Coleta dados de quadros e esculturas do site Leil√µesBR
"""

import re
import time
from typing import Optional, Dict, List
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from .base_scraper import BaseScraper


class LeiloesBRScraper(BaseScraper):
    """Scraper para coletar dados de quadros e esculturas do site Leil√µesBR"""
    
    def __init__(self, base_url: str = "https://leiloesbr.com.br", 
                 output_dir: str = "output", logs_dir: str = "logs", 
                 max_retries: int = 3, delay_between_requests: float = 1.0,
                 db_session=None, session_id: int = None):
        """
        Inicializa o scraper do Leil√µesBR
        
        Args:
            base_url: URL base do site
            output_dir: Diret√≥rio para salvar arquivos de sa√≠da
            logs_dir: Diret√≥rio para salvar logs
            max_retries: N√∫mero m√°ximo de tentativas por requisi√ß√£o
            delay_between_requests: Delay entre requisi√ß√µes (segundos)
            db_session: Sess√£o do banco de dados para verificar duplicatas
            session_id: ID da sess√£o de scraping
        """
        super().__init__(
            base_url=base_url,
            output_dir=output_dir,
            logs_dir=logs_dir,
            max_retries=max_retries,
            delay_between_requests=delay_between_requests,
            scraper_name="leiloes_br"
        )
        self.db_session = db_session
        self.session_id = session_id
        self._parar_scraping = False
        self.urls_coletadas = set()
        
        # URLs espec√≠ficas fornecidas pelo usu√°rio
        self.url_quadros = "https://leiloesbr.com.br/busca_andamento.asp?op=1&pesquisa=quadros&ga=*&uf=*&v=126&b=0&tp=|"
        self.url_esculturas = "https://leiloesbr.com.br/buscapos.asp?pesquisa=Esculturas"
    
    def parar_scraping(self):
        """Marca o scraping para parar"""
        self._parar_scraping = True
        self.logger.info("‚ö†Ô∏è Solicita√ß√£o de parada recebida. Finalizando ap√≥s salvar dados atuais...")
    
    def obra_ja_existe(self, url: str) -> bool:
        """Verifica se a obra j√° existe no banco de dados"""
        if not self.db_session:
            return False
        
        try:
            from database.models import Obra
            existe = self.db_session.query(Obra).filter(
                Obra.url == url,
                Obra.scraper_name == "leiloes_br"
            ).first()
            return existe is not None
        except Exception as e:
            self.logger.warning(f"Erro ao verificar duplicata: {e}")
            return False
    
    def descobrir_total_paginas(self, categoria: str = None) -> int:
        """Descobre o total de p√°ginas dispon√≠veis para uma categoria"""
        if categoria.lower() == "quadros":
            url_base = self.url_quadros
        elif categoria.lower() == "esculturas":
            url_base = self.url_esculturas
        else:
            self.logger.error(f"Categoria inv√°lida: {categoria}")
            return 0
        
        self.logger.info(f"Descobrindo total de p√°ginas para: {categoria}...")
        
        # Fazer requisi√ß√£o da primeira p√°gina
        response = self.fazer_requisicao(url_base)
        if not response:
            return 0
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Buscar pagina√ß√£o
        try:
            # Estrat√©gia 1: Contar itens e dividir por itens por p√°gina
            # Buscar "X Itens encontrados"
            itens_text = soup.find(text=re.compile(r'\d+\s+Itens encontrados', re.IGNORECASE))
            if itens_text:
                match = re.search(r'(\d+)\s+Itens encontrados', itens_text, re.IGNORECASE)
                if match:
                    total_itens = int(match.group(1))
                    # Buscar quantos itens por p√°gina (padr√£o: 126 ou 21)
                    visualizar_text = soup.find(text=re.compile(r'VISUALIZAR:', re.IGNORECASE))
                    itens_por_pagina = 126  # Padr√£o para quadros
                    if visualizar_text:
                        parent = visualizar_text.parent
                        if parent:
                            # Buscar select ou pr√≥ximo elemento com n√∫mero
                            select = parent.find_next('select')
                            if select:
                                option = select.find('option', selected=True)
                                if option:
                                    itens_por_pagina = int(option.get_text(strip=True))
                            else:
                                # Tentar pegar n√∫mero pr√≥ximo ao texto "VISUALIZAR:"
                                texto_parent = parent.get_text()
                                match_num = re.search(r'VISUALIZAR:\s*(\d+)', texto_parent, re.IGNORECASE)
                                if match_num:
                                    itens_por_pagina = int(match_num.group(1))
                    
                    total_paginas = (total_itens + itens_por_pagina - 1) // itens_por_pagina
                    self.logger.info(f"Total calculado: {total_paginas} p√°ginas ({total_itens} itens, {itens_por_pagina} por p√°gina)")
                    return total_paginas
            
            # Estrat√©gia 2: Buscar por "P√ÅGINA:" e encontrar os n√∫meros de p√°gina
            pagina_elements = soup.find_all(text=re.compile(r'P√ÅGINA|P√°gina', re.IGNORECASE))
            for elem in pagina_elements:
                parent = elem.parent
                if parent:
                    # Buscar todos os links e n√∫meros pr√≥ximos
                    # Pode estar em links <a> ou em spans/divs
                    max_pagina = 1
                    
                    # Buscar links de p√°gina
                    links_pagina = parent.find_all_next('a', href=True, limit=30)
                    for link in links_pagina:
                        texto = link.get_text(strip=True)
                        if texto.isdigit():
                            num = int(texto)
                            if num > max_pagina and num < 1000:  # Limite razo√°vel
                                max_pagina = num
                    
                    # Buscar n√∫meros no texto pr√≥ximo
                    texto_parent = parent.get_text()
                    numeros = re.findall(r'\b(\d+)\b', texto_parent)
                    for num_str in numeros:
                        num = int(num_str)
                        if num > max_pagina and num < 1000:
                            max_pagina = num
                    
                    if max_pagina > 1:
                        self.logger.info(f"Total de p√°ginas encontrado: {max_pagina}")
                        return max_pagina
        except Exception as e:
            self.logger.warning(f"Erro ao descobrir p√°ginas: {e}")
        
        # Se n√£o conseguir descobrir, retornar 1 (pelo menos uma p√°gina)
        self.logger.warning("N√£o foi poss√≠vel descobrir total de p√°ginas, usando 1")
        return 1
    
    def processar_pagina(self, url: str, numero_pagina: int, categoria: str = None):
        """Processa uma p√°gina espec√≠fica e extrai dados das obras"""
        if self._parar_scraping:
            return
        
        self.logger.info(f"Processando p√°gina {numero_pagina}: {url}")
        
        response = self.fazer_requisicao(url)
        if not response:
            self.logger.error(f"Erro ao acessar p√°gina {numero_pagina}")
            return
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Buscar obras na p√°gina - geralmente est√£o em divs com classes espec√≠ficas
        obras = self._encontrar_obras_na_pagina(soup)
        
        self.logger.info(f"Encontradas {len(obras)} obras na p√°gina {numero_pagina}")
        
        # Processar cada obra encontrada
        total_obras = len(obras)
        self.logger.info(f"üì¶ Encontradas {total_obras} obras na p√°gina {numero_pagina}")
        
        if total_obras == 0:
            self.logger.warning("‚ö†Ô∏è Nenhuma obra encontrada na p√°gina!")
            return
        
        obras_processadas = 0
        obras_puladas = 0
        
        for i, obra_data in enumerate(obras, 1):
            if self._parar_scraping:
                break
            
            try:
                # Log de progresso a cada 5 obras ou na primeira
                if i % 5 == 0 or i == 1:
                    self.logger.info(f"  ‚è≥ Progresso: {i}/{total_obras} obras | Processadas: {obras_processadas} | Puladas: {obras_puladas}")
                
                # Verificar se j√° existe antes de fazer requisi√ß√£o (mais r√°pido)
                url_obra = obra_data.get('url', '')
                if url_obra and self.obra_ja_existe(url_obra):
                    obras_puladas += 1
                    self.urls_coletadas.add(url_obra)
                    continue
                
                self.processar_obra_da_listagem(obra_data, numero_pagina, categoria)
                obras_processadas += 1
                
                # Delay reduzido entre requisi√ß√µes (0.3s ao inv√©s de 1s para ser mais r√°pido)
                time.sleep(0.3)
            except Exception as e:
                self.logger.error(f"  ‚ùå Erro ao processar obra {i}: {e}")
                continue
        
        self.logger.info(f"‚úÖ P√°gina {numero_pagina} conclu√≠da: {obras_processadas} novas, {obras_puladas} puladas")
    
    def _encontrar_obras_na_pagina(self, soup: BeautifulSoup) -> List[Dict]:
        """Encontra todas as obras na p√°gina de listagem"""
        obras = []
        
        try:
            # Estrat√©gia 1: Buscar por divs com classes de produto/card (mais confi√°vel)
            # Baseado na estrutura do site, os cards t√™m classes espec√≠ficas
            cards = soup.find_all(['div', 'article', 'section'], 
                                class_=lambda x: x and any(
                                    keyword in str(x).lower() 
                                    for keyword in ['product', 'item', 'card', 'peca', 'obra', 'lote', 'grid']
                                ))
            
            for card in cards:
                # Buscar link dentro do card
                link = card.find('a', href=re.compile(r'peca\.asp|item\.asp|lote|busca', re.IGNORECASE))
                if not link:
                    # Tentar buscar qualquer link no card
                    link = card.find('a', href=True)
                    if not link:
                        continue
                
                href = link.get('href', '')
                if not href:
                    continue
                
                # Filtrar links que n√£o s√£o de obras
                if any(skip in href.lower() for skip in ['busca', 'categoria', 'filtro', 'pagina']):
                    continue
                
                # Normalizar URL
                if href.startswith('/'):
                    href = urljoin(self.base_url, href)
                elif not href.startswith('http'):
                    href = urljoin(self.base_url, '/' + href)
                
                # Extrair dados b√°sicos do card
                titulo = self._extrair_titulo_do_card(card, link)
                valor = self._extrair_valor_do_card(card)
                imagem_url = self._extrair_imagem_do_card(card)
                data_leilao = self._extrair_data_leilao_do_card(card)
                leiloeiro = self._extrair_leiloeiro_do_card(card)
                
                # Verificar se j√° foi adicionada (evitar duplicatas)
                if href not in [o.get('url') for o in obras]:
                    obras.append({
                        'url': href,
                        'titulo': titulo,
                        'valor': valor,
                        'imagem': imagem_url,
                        'data_leilao': data_leilao,
                        'leiloeiro': leiloeiro,
                        'card_element': card
                    })
            
            # Estrat√©gia 2: Se n√£o encontrou nada, buscar por links diretos
            if not obras:
                links_pecas = soup.find_all('a', href=re.compile(r'peca\.asp|item\.asp', re.IGNORECASE))
                
                for link in links_pecas:
                    href = link.get('href', '')
                    if not href:
                        continue
                    
                    # Normalizar URL
                    if href.startswith('/'):
                        href = urljoin(self.base_url, href)
                    elif not href.startswith('http'):
                        href = urljoin(self.base_url, '/' + href)
                    
                    # Buscar informa√ß√µes da obra no elemento pai
                    parent_card = link.find_parent(['div', 'article', 'section'])
                    if not parent_card:
                        continue
                    
                    titulo = self._extrair_titulo_do_card(parent_card, link)
                    valor = self._extrair_valor_do_card(parent_card)
                    imagem_url = self._extrair_imagem_do_card(parent_card)
                    
                    if href not in [o.get('url') for o in obras]:
                        obras.append({
                            'url': href,
                            'titulo': titulo,
                            'valor': valor,
                            'imagem': imagem_url,
                            'card_element': parent_card
                        })
        
        except Exception as e:
            self.logger.error(f"Erro ao encontrar obras na p√°gina: {e}")
        
        return obras
    
    def _extrair_titulo_do_card(self, card, link) -> str:
        """Extrai o t√≠tulo da obra do card da listagem"""
        try:
            # Estrat√©gia 1: Buscar em divs com classes de t√≠tulo/nome
            titulo_divs = card.find_all(['div', 'span', 'p'], 
                                      class_=lambda x: x and any(
                                          keyword in str(x).lower() 
                                          for keyword in ['title', 'titulo', 'nome', 'name', 'product-title']
                                      ))
            for div in titulo_divs:
                titulo = div.get_text(strip=True)
                # Remover quebras de linha e espa√ßos extras
                titulo = ' '.join(titulo.split())
                if titulo and len(titulo) > 10:
                    return titulo
            
            # Estrat√©gia 2: Buscar em h2, h3, h4 dentro do card
            for tag in ['h2', 'h3', 'h4', 'h5']:
                heading = card.find(tag)
                if heading:
                    titulo = heading.get_text(strip=True)
                    titulo = ' '.join(titulo.split())
                    if titulo and len(titulo) > 10:
                        return titulo
            
            # Estrat√©gia 3: Tentar pegar do texto do link
            titulo = link.get_text(strip=True)
            titulo = ' '.join(titulo.split())
            if titulo and len(titulo) > 10:
                return titulo
            
            # Estrat√©gia 4: Buscar em qualquer elemento dentro do card que tenha texto longo
            # Mas n√£o seja pre√ßo, data, etc.
            elementos = card.find_all(['div', 'span', 'p', 'a'])
            for elem in elementos:
                texto = elem.get_text(strip=True)
                texto = ' '.join(texto.split())
                # Verificar se n√£o √© pre√ßo, data, ou texto muito curto
                if (len(texto) > 15 and 
                    not re.search(r'R\$\s*[\d.,]+', texto) and
                    not re.search(r'\d{2}/\d{2}/\d{4}', texto) and
                    'leil√£o' not in texto.lower() and
                    'leiloeiro' not in texto.lower()):
                    return texto
        
        except Exception as e:
            self.logger.debug(f"Erro ao extrair t√≠tulo do card: {e}")
        
        return "N/A"
    
    def _extrair_valor_do_card(self, card) -> str:
        """Extrai o valor da obra do card da listagem"""
        try:
            # Estrat√©gia 1: Buscar em elementos com classes espec√≠ficas de pre√ßo
            # Baseado na imagem: class="product-price venda-price"
            valor_elements = card.find_all(['div', 'span', 'strong', 'p'], 
                                         class_=lambda x: x and any(
                                             keyword in str(x).lower() 
                                             for keyword in ['price', 'valor', 'preco', 'venda']
                                         ))
            for elem in valor_elements:
                texto = elem.get_text(strip=True)
                # Buscar padr√£o R$ seguido de n√∫meros
                match = re.search(r'R\$\s*([\d.,]+)', texto)
                if match:
                    valor = match.group(1)
                    # Validar que √© um valor razo√°vel
                    valor_num = valor.replace('.', '').replace(',', '.')
                    try:
                        if float(valor_num) > 0:
                            return valor
                    except:
                        pass
            
            # Estrat√©gia 2: Buscar por padr√£o R$ seguido de n√∫meros no texto do card
            texto_card = card.get_text()
            matches = list(re.finditer(r'R\$\s*([\d.,]+)', texto_card))
            if matches:
                # Pegar o primeiro valor encontrado (geralmente √© o valor da obra)
                for match in matches:
                    valor = match.group(1)
                    valor_num = valor.replace('.', '').replace(',', '.')
                    try:
                        if float(valor_num) > 0:
                            return valor
                    except:
                        continue
        
        except Exception as e:
            self.logger.debug(f"Erro ao extrair valor do card: {e}")
        
        return "N/A"
    
    def _extrair_imagem_do_card(self, card) -> str:
        """Extrai a URL da imagem do card"""
        try:
            img = card.find('img', src=True)
            if img:
                src = img.get('src', '')
                if src.startswith('/'):
                    return urljoin(self.base_url, src)
                elif not src.startswith('http'):
                    return urljoin(self.base_url, '/' + src)
                return src
        except:
            pass
        return ""
    
    def _extrair_data_leilao_do_card(self, card) -> str:
        """Extrai a data/hora do leil√£o do card"""
        try:
            texto_card = card.get_text()
            # Padr√£o: "DD/MM/YYYY - HHh" ou "DD/MM/YYYY HH:MM"
            match = re.search(r'(\d{2}/\d{2}/\d{4})\s*[-‚Äì]\s*(\d{1,2})h', texto_card)
            if match:
                return f"{match.group(1)} {match.group(2)}:00"
            
            match = re.search(r'(\d{2}/\d{2}/\d{4})\s+(\d{2}:\d{2})', texto_card)
            if match:
                return f"{match.group(1)} {match.group(2)}"
        except:
            pass
        return ""
    
    def _extrair_leiloeiro_do_card(self, card) -> str:
        """Extrai o nome do leiloeiro do card"""
        try:
            texto_card = card.get_text()
            # Buscar padr√µes comuns de nomes de leiloeiros
            # Geralmente aparece no final do card
            leiloeiro_elements = card.find_all(['div', 'span', 'p'], 
                                              class_=re.compile(r'leiloeiro|seller|vendedor', re.IGNORECASE))
            for elem in leiloeiro_elements:
                texto = elem.get_text(strip=True)
                if texto and len(texto) > 5:
                    return texto
        except:
            pass
        return ""
    
    def processar_obra(self, url_obra: str, numero_pagina: int, categoria: str = None):
        """Processa uma obra espec√≠fica e extrai seus dados (m√©todo abstrato requerido)"""
        # Wrapper para processar_obra_da_listagem
        obra_data = {
            'url': url_obra,
            'titulo': 'N/A',
            'valor': 'N/A',
            'imagem': '',
            'data_leilao': '',
            'leiloeiro': ''
        }
        self.processar_obra_da_listagem(obra_data, numero_pagina, categoria)
    
    def processar_obra_da_listagem(self, obra_data: Dict, numero_pagina: int, categoria: str):
        """Processa uma obra encontrada na listagem"""
        url_obra = obra_data.get('url', '')
        if not url_obra:
            return
        
        # Verificar se j√° foi coletada nesta execu√ß√£o (mesma regra do iArremate)
        if url_obra in self.urls_coletadas:
            self.logger.debug(f"    ‚äò Obra j√° coletada nesta execu√ß√£o: {url_obra}")
            return
        
        # Verificar se j√° existe no banco ANTES de fazer requisi√ß√£o (mesma regra do iArremate)
        if self.obra_ja_existe(url_obra):
            self.logger.info(f"    ‚äò Obra j√° existe no banco (pulando): {url_obra}")
            self.urls_coletadas.add(url_obra)  # Adicionar ao cache para n√£o verificar novamente
            return
        
        # Fazer requisi√ß√£o para a p√°gina da obra (pode redirecionar)
        response = self.fazer_requisicao(url_obra)
        if not response:
            return
        
        url_final = response.url
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extrair dados da p√°gina da obra
        # Usar dados do card como fallback se dispon√≠veis
        titulo_card = obra_data.get('titulo', 'N/A')
        valor_card = obra_data.get('valor', 'N/A')
        
        titulo = self.extrair_titulo_leiloes_br(soup, titulo_card)
        descricao = self.extrair_descricao_leiloes_br(soup, titulo)
        nome_artista = self.extrair_nome_artista(titulo, descricao)
        valor = self.extrair_valor_leiloes_br(soup, valor_card)
        
        # VERIFICAR SE TEM VALOR - IGNORAR SE N√ÉO TIVER
        if not valor or valor == "N/A":
            self.logger.info(f"    ‚äò Obra sem valor (ignorando): {url_obra}")
            self.urls_coletadas.add(url_obra)  # Adicionar ao cache para n√£o processar novamente
            return
        
        # Validar que o valor √© um n√∫mero v√°lido
        try:
            valor_limpo = valor.replace('R$', '').replace('.', '').replace(',', '.').strip()
            valor_float = float(valor_limpo)
            if valor_float <= 0:
                self.logger.info(f"    ‚äò Obra com valor inv√°lido (ignorando): {url_obra} - Valor: {valor}")
                self.urls_coletadas.add(url_obra)
                return
        except (ValueError, AttributeError):
            self.logger.info(f"    ‚äò Obra com valor inv√°lido (ignorando): {url_obra} - Valor: {valor}")
            self.urls_coletadas.add(url_obra)
            return
        
        lote = self.extrair_lote_leiloes_br(soup, url_final)
        
        # Usar data do card se dispon√≠vel, sen√£o extrair da p√°gina
        data_inicio_leilao = obra_data.get('data_leilao', '')
        if not data_inicio_leilao:
            data_inicio_leilao = self.extrair_data_inicio_leilao_leiloes_br(soup)
        
        # Informa√ß√µes adicionais
        leiloeiro = obra_data.get('leiloeiro', '')
        if not leiloeiro or leiloeiro == '':
            leiloeiro = self.extrair_leiloeiro_leiloes_br(soup)
        
        local = self.extrair_local_leiloes_br(soup)
        data_leilao = self.extrair_data_leilao_leiloes_br(soup)
        
        # Determinar categoria
        categoria_final = categoria or "Quadros"
        
        # Log detalhado para debug
        self.logger.debug(f"    üìã Dados extra√≠dos:")
        self.logger.debug(f"       T√≠tulo: {titulo}")
        self.logger.debug(f"       Artista: {nome_artista}")
        self.logger.debug(f"       Valor: {valor}")
        self.logger.debug(f"       Lote: {lote}")
        self.logger.debug(f"       Data In√≠cio: {data_inicio_leilao}")
        self.logger.debug(f"       Data Leil√£o: {data_leilao}")
        self.logger.debug(f"       Leiloeiro: {leiloeiro}")
        self.logger.debug(f"       Local: {local}")
        
        # Criar entrada de dados
        dados_obra = {
            'Nome_Artista': nome_artista,
            'Categoria': categoria_final,
            'Pagina': numero_pagina,
            'Titulo': titulo,
            'Descricao': descricao,
            'Valor': valor,
            'Lote': lote,
            'Data_Inicio_Leilao': data_inicio_leilao,
            'Data_Leilao': data_leilao,
            'Leiloeiro': leiloeiro,
            'Local': local,
            'URL': url_final,
            'URL_Original': url_obra,
            'Site_Redirecionado': self._extrair_dominio_redirecionado(url_final) if url_final != url_obra else "N/A",
            'Data_Coleta': time.strftime('%d/%m/%Y %H:%M:%S')
        }
        
        self.dados_obras.append(dados_obra)
        self.urls_coletadas.add(url_obra)  # Adicionar ao cache ap√≥s coletar (mesma regra do iArremate)
        self.logger.info(f"    ‚úì Obra coletada ({categoria_final}): {nome_artista} - Valor: R$ {valor} | Lote: {lote} | Leiloeiro: {leiloeiro}")
    
    def _extrair_dominio_redirecionado(self, url: str) -> str:
        """Extrai o dom√≠nio de uma URL"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    
    def extrair_titulo_leiloes_br(self, soup: BeautifulSoup, titulo_listagem: str = "N/A") -> str:
        """Extrai o t√≠tulo da obra da p√°gina individual"""
        # N√ÉO usar t√≠tulo da listagem se for "Lotes relacionados" ou similar
        if titulo_listagem and titulo_listagem != "N/A":
            # Ignorar t√≠tulos gen√©ricos como "Lotes relacionados"
            if 'lotes relacionados' not in titulo_listagem.lower():
                return titulo_listagem
        
        try:
            # ESTRAT√âGIA 1: Buscar em div.lote-desc (Miguel Salles espec√≠fico)
            # O t√≠tulo real est√° em <div class="lote-desc text-list"> <p>...</p>
            lote_desc_divs = soup.find_all('div', class_=re.compile(r'lote-desc', re.IGNORECASE))
            for div in lote_desc_divs:
                # Buscar par√°grafo dentro do div
                paragrafo = div.find('p')
                if paragrafo:
                    titulo = paragrafo.get_text(strip=True)
                    # Validar: deve ser uma descri√ß√£o completa da obra (n√£o s√≥ "Lote X")
                    if (titulo and len(titulo) > 20 and 
                        'lotes relacionados' not in titulo.lower() and
                        not titulo.lower().startswith('lote') and
                        not re.match(r'^[\d\sR$.,:LoteVisitasLance]+$', titulo, re.IGNORECASE)):
                        return titulo
                # Se n√£o tiver <p>, pegar texto do div
                else:
                    titulo = div.get_text(strip=True)
                    if (titulo and len(titulo) > 20 and 
                        'lotes relacionados' not in titulo.lower()):
                        return titulo
            
            # ESTRAT√âGIA 2: Buscar em divs com classes espec√≠ficas de descri√ß√£o
            # Miguel Salles e Roberto Haddad usam classes como: is-pecadesc, product-description, lote-desc, etc.
            desc_divs = soup.find_all(['div', 'span', 'p'], 
                                     class_=re.compile(r'desc|description|is-pecadesc|product-description|lote-desc|text-list', re.IGNORECASE))
            for div in desc_divs:
                titulo = div.get_text(strip=True)
                # Validar t√≠tulo: deve ter mais de 20 caracteres e n√£o ser gen√©rico
                if (titulo and len(titulo) > 20 and 
                    'lotes relacionados' not in titulo.lower() and
                    not titulo.lower().startswith('lote') and
                    not re.match(r'^[\d\sR$.,:LoteVisitasLance]+$', titulo, re.IGNORECASE)):
                    return titulo
            
            # ESTRAT√âGIA 3: Buscar em h1, h2, h3 (mas validar melhor)
            for tag in ['h1', 'h2', 'h3']:
                headings = soup.find_all(tag)
                for heading in headings:
                    titulo = heading.get_text(strip=True)
                    # Validar t√≠tulo: deve ter mais de 10 caracteres e n√£o ser gen√©rico
                    if (titulo and len(titulo) > 10 and 
                        'lotes relacionados' not in titulo.lower() and
                        not titulo.lower().startswith('lote') and
                        'lote' not in titulo.lower()[:20]):  # Ignorar se come√ßar com "Lote"
                        return titulo
            
            # ESTRAT√âGIA 4: Buscar pr√≥ximo ao texto "PE√áA" ou "Tipo:"
            # Geralmente o t√≠tulo aparece pr√≥ximo a esses textos
            peca_elem = soup.find(text=re.compile(r'PE√áA|Tipo:', re.IGNORECASE))
            if peca_elem:
                parent = peca_elem.parent
                if parent:
                    # Buscar pr√≥ximo elemento com descri√ß√£o
                    next_elem = parent.find_next(['div', 'p', 'span'], 
                                                class_=re.compile(r'desc|description|lote-desc', re.IGNORECASE))
                    if next_elem:
                        titulo = next_elem.get_text(strip=True)
                        if (titulo and len(titulo) > 20 and 
                            'lotes relacionados' not in titulo.lower()):
                            return titulo
            
            # ESTRAT√âGIA 5: Buscar tag title e limpar
            title_tag = soup.find('title')
            if title_tag:
                titulo = title_tag.get_text(strip=True)
                # Limpar se tiver " - Leil√µesBR" ou similar
                if 'leil√µes' in titulo.lower() or 'leiloes' in titulo.lower():
                    partes = re.split(r'\s*-\s*', titulo)
                    for parte in partes:
                        parte = parte.strip()
                        if (len(parte) > 10 and 
                            'leil√µes' not in parte.lower() and
                            'lotes relacionados' not in parte.lower()):
                            return parte
                # Se n√£o tiver "leil√µes", usar direto se for v√°lido
                if len(titulo) > 10 and 'lotes relacionados' not in titulo.lower():
                    return titulo
        
        except Exception as e:
            self.logger.debug(f"Erro ao extrair t√≠tulo: {e}")
        
        return "N/A"
    
    def extrair_descricao_leiloes_br(self, soup: BeautifulSoup, titulo: str = None) -> str:
        """Extrai a descri√ß√£o (usa o t√≠tulo se dispon√≠vel)"""
        if titulo and titulo != "N/A":
            return titulo
        return "N/A"
    
    def extrair_valor_leiloes_br(self, soup: BeautifulSoup, valor_listagem: str = "N/A") -> str:
        """Extrai o valor da obra"""
        if valor_listagem and valor_listagem != "N/A":
            return valor_listagem
        
        try:
            # Buscar por padr√£o R$ seguido de n√∫meros
            texto_completo = soup.get_text()
            matches = list(re.finditer(r'R\$\s*([\d.,]+)', texto_completo))
            if matches:
                # Pegar o primeiro valor encontrado (geralmente √© o valor atual)
                return matches[0].group(1)
            
            # Buscar em elementos com classes de pre√ßo
            valor_elements = soup.find_all(['div', 'span', 'strong'], 
                                         class_=re.compile(r'price|valor|preco|venda', re.IGNORECASE))
            for elem in valor_elements:
                texto = elem.get_text(strip=True)
                match = re.search(r'R\$\s*([\d.,]+)|([\d.,]+)', texto)
                if match:
                    valor = match.group(1) or match.group(2)
                    # Validar que √© um valor razo√°vel
                    valor_num = valor.replace('.', '').replace(',', '.')
                    try:
                        if float(valor_num) > 10:
                            return valor
                    except:
                        pass
        
        except Exception as e:
            self.logger.debug(f"Erro ao extrair valor: {e}")
        
        return "N/A"
    
    def extrair_lote_leiloes_br(self, soup: BeautifulSoup, url: str) -> str:
        """Extrai o n√∫mero do lote (pode estar no site redirecionado)"""
        lote = "N/A"
        
        try:
            # Estrat√©gia 1: Buscar em breadcrumbs (mais confi√°vel em sites redirecionados)
            # Exemplo: "HOME > LISTA DE CAT√ÅLOGOS > LEIL√ÉO 55780 > CAT√ÅLOGO DE PE√áAS > LOTE 20"
            breadcrumbs = soup.find_all(['div', 'nav', 'section'], 
                                      class_=lambda x: x and any(
                                          keyword in str(x).lower() 
                                          for keyword in ['breadcrumb', 'navegacao', 'navigation']
                                      ))
            for breadcrumb in breadcrumbs:
                texto = breadcrumb.get_text()
                match = re.search(r'Lote\s+(\d+)', texto, re.IGNORECASE)
                if match:
                    lote = match.group(1)
                    return lote
            
            # Estrat√©gia 2: Buscar no t√≠tulo (ex: "Lote 20" no t√≠tulo)
            title_tag = soup.find('title')
            if title_tag:
                titulo = title_tag.get_text()
                match = re.search(r'Lote\s+(\d+)', titulo, re.IGNORECASE)
                if match:
                    lote = match.group(1)
                    return lote
            
            # Estrat√©gia 3: Buscar em h1, h2, h3 que contenham "Lote"
            for tag in ['h1', 'h2', 'h3', 'h4']:
                headings = soup.find_all(tag)
                for heading in headings:
                    texto = heading.get_text()
                    match = re.search(r'Lote\s+(\d+)', texto, re.IGNORECASE)
                    if match:
                        lote = match.group(1)
                        return lote
            
            # Estrat√©gia 4: Buscar por "Lote" seguido de n√∫mero em qualquer texto
            textos_lote = soup.find_all(text=re.compile(r'Lote\s+\d+|LOTE\s+\d+', re.IGNORECASE))
            for texto in textos_lote:
                match = re.search(r'Lote\s+(\d+)', str(texto), re.IGNORECASE)
                if match:
                    lote = match.group(1)
                    return lote
            
            # Estrat√©gia 5: Buscar em elementos com classe "lote" ou similar
            lote_elements = soup.find_all(['div', 'span', 'strong', 'h1', 'h2', 'h3'], 
                                        class_=lambda x: x and 'lote' in str(x).lower())
            for elem in lote_elements:
                texto = elem.get_text(strip=True)
                # Se for apenas um n√∫mero, pode ser o lote
                if re.match(r'^\d+$', texto):
                    lote = texto
                    return lote
                # Ou "Lote X"
                match = re.search(r'Lote\s+(\d+)', texto, re.IGNORECASE)
                if match:
                    lote = match.group(1)
                    return lote
            
            # Estrat√©gia 6: Buscar em tabelas (comum em sites de leil√£o)
            tabelas = soup.find_all('table')
            for tabela in tabelas:
                linhas = tabela.find_all('tr')
                for linha in linhas:
                    texto_linha = linha.get_text()
                    if re.search(r'lote', texto_linha, re.IGNORECASE):
                        tds = linha.find_all('td')
                        if len(tds) >= 2:
                            # O segundo td geralmente tem o n√∫mero do lote
                            texto_td = tds[1].get_text(strip=True)
                            match = re.search(r'(\d+)', texto_td)
                            if match:
                                lote = match.group(1)
                                return lote
                        # Ou buscar "Lote X" na linha
                        match = re.search(r'Lote\s+(\d+)', texto_linha, re.IGNORECASE)
                        if match:
                            lote = match.group(1)
                            return lote
            
            # Estrat√©gia 7: Buscar padr√£o "Lote X" no texto completo da p√°gina
            texto_completo = soup.get_text()
            matches = list(re.finditer(r'Lote\s+(\d+)', texto_completo, re.IGNORECASE))
            if matches:
                # Pegar o primeiro match (geralmente √© o lote da obra)
                lote = matches[0].group(1)
                return lote
            
            # Estrat√©gia 8: Buscar apenas n√∫meros que podem ser lote (em contexto de leil√£o)
            # Buscar em elementos que contenham "lote" no texto pr√≥ximo
            elementos_com_lote = soup.find_all(['div', 'span', 'p', 'td'], 
                                              string=re.compile(r'\d+', re.IGNORECASE))
            for elem in elementos_com_lote:
                parent = elem.parent
                if parent:
                    texto_parent = parent.get_text()
                    if re.search(r'lote', texto_parent, re.IGNORECASE):
                        numero = elem.get_text(strip=True)
                        if re.match(r'^\d+$', numero) and 1 <= int(numero) <= 10000:
                            lote = numero
                            return lote
        
        except Exception as e:
            self.logger.debug(f"Erro ao extrair lote: {e}")
        
        return lote if lote != "N/A" else "N/A"
    
    def extrair_data_inicio_leilao_leiloes_br(self, soup: BeautifulSoup) -> str:
        """Extrai a data/hora de in√≠cio do leil√£o - melhorado"""
        data_inicio = "nao tem"
        
        try:
            # Estrat√©gia 1: Buscar por "DIA DO LEIL√ÉO", "In√≠cio", "Data do Leil√£o"
            keywords = ['dia do leil√£o', 'in√≠cio', 'inicio', 'data do leil√£o', 'data do leilao', 'hor√°rio', 'horario']
            for keyword in keywords:
                textos = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
                for texto in textos:
                    parent = texto.parent
                    if parent:
                        texto_completo = parent.get_text()
                        # Padr√£o: DD/MM/YYYY - HHh ou DD/MM/YYYY HH:MM
                        match = re.search(r'(\d{2}/\d{2}/\d{4})\s*[-‚Äì]\s*(\d{1,2})h', texto_completo)
                        if match:
                            data_inicio = f"{match.group(1)} {match.group(2)}:00"
                            return data_inicio
                        
                        match = re.search(r'(\d{2}/\d{2}/\d{4})\s+(\d{2}:\d{2})', texto_completo)
                        if match:
                            data_inicio = f"{match.group(1)} {match.group(2)}"
                            return data_inicio
                        
                        # Apenas data sem hora
                        match = re.search(r'(\d{2}/\d{2}/\d{4})', texto_completo)
                        if match:
                            data_inicio = match.group(1)
                            return data_inicio
            
            # Estrat√©gia 2: Buscar em elementos com classes relacionadas a data/hora
            data_elements = soup.find_all(['div', 'span', 'td', 'p'], 
                                        class_=lambda x: x and any(
                                            keyword in str(x).lower() 
                                            for keyword in ['data', 'hora', 'horario', 'leilao', 'auction', 'inicio', 'in√≠cio']
                                        ))
            for elem in data_elements:
                texto = elem.get_text(strip=True)
                # Padr√£o com hora: DD/MM/YYYY - HHh
                match = re.search(r'(\d{2}/\d{2}/\d{4})\s*[-‚Äì]\s*(\d{1,2})h', texto)
                if match:
                    data_inicio = f"{match.group(1)} {match.group(2)}:00"
                    return data_inicio
                
                # Padr√£o com hora completa: DD/MM/YYYY HH:MM
                match = re.search(r'(\d{2}/\d{2}/\d{4})\s+(\d{2}:\d{2})', texto)
                if match:
                    data_inicio = f"{match.group(1)} {match.group(2)}"
                    return data_inicio
            
            # Estrat√©gia 3: Buscar em tabelas
            tabelas = soup.find_all('table')
            for tabela in tabelas:
                linhas = tabela.find_all('tr')
                for linha in linhas:
                    texto_linha = linha.get_text()
                    if re.search(r'data|in√≠cio|inicio|leil√£o', texto_linha, re.IGNORECASE):
                        tds = linha.find_all('td')
                        if len(tds) >= 2:
                            texto_td = tds[1].get_text(strip=True)
                            match = re.search(r'(\d{2}/\d{2}/\d{4})(?:\s+(\d{2}:\d{2})|\s*[-‚Äì]\s*(\d{1,2})h)?', texto_td)
                            if match:
                                data = match.group(1)
                                hora = match.group(2) or (match.group(3) + ":00" if match.group(3) else "")
                                if hora:
                                    data_inicio = f"{data} {hora}"
                                else:
                                    data_inicio = data
                                return data_inicio
        
        except Exception as e:
            self.logger.debug(f"Erro ao extrair data de in√≠cio do leil√£o: {e}")
        
        return data_inicio if data_inicio != "nao tem" else "nao tem"
    
    def extrair_data_leilao_leiloes_br(self, soup: BeautifulSoup) -> str:
        """Extrai a data do leil√£o (formato simples) - melhorado"""
        try:
            # Estrat√©gia 1: Buscar em elementos com classes relacionadas a data
            data_elements = soup.find_all(['div', 'span', 'td', 'p'], 
                                        class_=lambda x: x and any(
                                            keyword in str(x).lower() 
                                            for keyword in ['data', 'date', 'leilao', 'auction', 'dia']
                                        ))
            for elem in data_elements:
                texto = elem.get_text(strip=True)
                # Buscar padr√£o DD/MM/YYYY
                match = re.search(r'(\d{2}/\d{2}/\d{4})', texto)
                if match:
                    return match.group(1)
            
            # Estrat√©gia 2: Buscar pr√≥ximo a palavras-chave
            keywords = ['data', 'leil√£o', 'leilao', 'dia', 'realiza√ß√£o']
            for keyword in keywords:
                textos = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
                for texto in textos:
                    parent = texto.parent
                    if parent:
                        texto_completo = parent.get_text()
                        match = re.search(r'(\d{2}/\d{2}/\d{4})', texto_completo)
                        if match:
                            return match.group(1)
            
            # Estrat√©gia 3: Buscar qualquer data no texto completo (√∫ltima tentativa)
            texto_completo = soup.get_text()
            matches = list(re.finditer(r'(\d{2}/\d{2}/\d{4})', texto_completo))
            if matches:
                # Pegar a primeira data encontrada (geralmente √© a do leil√£o)
                return matches[0].group(1)
        except Exception as e:
            self.logger.debug(f"Erro ao extrair data do leil√£o: {e}")
        return "N/A"
    
    def extrair_leiloeiro_leiloes_br(self, soup: BeautifulSoup) -> str:
        """Extrai o nome do leiloeiro - melhorado"""
        try:
            # Estrat√©gia 1: Buscar em elementos com classes espec√≠ficas
            leiloeiro_elements = soup.find_all(['div', 'span', 'td', 'p'], 
                                              class_=lambda x: x and any(
                                                  keyword in str(x).lower() 
                                                  for keyword in ['leiloeiro', 'seller', 'vendedor', 'escritorio', 'auctioneer']
                                              ))
            for elem in leiloeiro_elements:
                texto = elem.get_text(strip=True)
                # Remover a palavra "Leiloeiro:" ou similar
                texto_limpo = re.sub(r'^(?:Leiloeiro|Leiloeira|Escrit√≥rio)[:\s]+', '', texto, flags=re.IGNORECASE)
                texto_limpo = texto_limpo.strip()
                if texto_limpo and len(texto_limpo) > 3 and len(texto_limpo) < 100:
                    return texto_limpo
            
            # Estrat√©gia 2: Buscar pr√≥ximo a palavras-chave
            keywords = ['leiloeiro', 'leiloeira', 'escrit√≥rio', 'escritorio', 'leil√£o por']
            for keyword in keywords:
                textos = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
                for texto in textos:
                    parent = texto.parent
                    if parent:
                        texto_completo = parent.get_text()
                        # Padr√£o: "Leiloeiro: Nome" ou "Leiloeiro Nome"
                        match = re.search(
                            r'(?:Leiloeiro|Leiloeira|Escrit√≥rio|Escritorio)[:\s]+([A-Z√Å√â√ç√ì√ö√á][a-z√°√©√≠√≥√∫√ß]+(?:\s+[A-Z√Å√â√ç√ì√ö√á][a-z√°√©√≠√≥√∫√ß]+)*)',
                            texto_completo,
                            re.IGNORECASE
                        )
                        if match:
                            nome = match.group(1).strip()
                            if len(nome) > 3 and len(nome) < 100:
                                return nome
                        
                        # Tentar pegar texto ap√≥s os dois pontos
                        if ':' in texto_completo:
                            partes = texto_completo.split(':')
                            if len(partes) > 1:
                                nome = partes[1].strip()
                                # Limpar nome (remover datas, n√∫meros, etc)
                                nome = re.sub(r'\d{2}/\d{2}/\d{4}.*$', '', nome).strip()
                                nome = re.sub(r'\s+', ' ', nome)
                                if len(nome) > 3 and len(nome) < 100 and not re.match(r'^\d+$', nome):
                                    return nome
            
            # Estrat√©gia 3: Buscar em tabelas (geralmente tem informa√ß√µes estruturadas)
            tabelas = soup.find_all('table')
            for tabela in tabelas:
                linhas = tabela.find_all('tr')
                for linha in linhas:
                    texto_linha = linha.get_text()
                    if re.search(r'leiloeiro|leiloeira', texto_linha, re.IGNORECASE):
                        tds = linha.find_all('td')
                        if len(tds) >= 2:
                            # O segundo td geralmente tem o nome
                            nome = tds[1].get_text(strip=True)
                            if len(nome) > 3 and len(nome) < 100:
                                return nome
        except Exception as e:
            self.logger.debug(f"Erro ao extrair leiloeiro: {e}")
        return "N/A"
    
    def extrair_local_leiloes_br(self, soup: BeautifulSoup) -> str:
        """Extrai o local do leil√£o - melhorado"""
        try:
            # Estrat√©gia 1: Buscar em elementos com classes espec√≠ficas
            local_elements = soup.find_all(['div', 'span', 'td', 'p'], 
                                         class_=lambda x: x and any(
                                             keyword in str(x).lower() 
                                             for keyword in ['local', 'location', 'cidade', 'city', 'endereco']
                                         ))
            for elem in local_elements:
                texto = elem.get_text(strip=True)
                # Remover a palavra "Local:" ou similar
                texto_limpo = re.sub(r'^(?:Local|LOCAL|Cidade|Endere√ßo)[:\s]+', '', texto, flags=re.IGNORECASE)
                texto_limpo = texto_limpo.strip()
                if texto_limpo and len(texto_limpo) > 3 and len(texto_limpo) < 100:
                    return texto_limpo
            
            # Estrat√©gia 2: Buscar pr√≥ximo a palavras-chave
            keywords = ['local', 'cidade', 'endere√ßo', 'endereco', 'realiza√ß√£o']
            for keyword in keywords:
                textos = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
                for texto in textos:
                    parent = texto.parent
                    if parent:
                        texto_completo = parent.get_text()
                        # Padr√£o: "Local: Cidade - Estado" ou "Cidade - Estado"
                        match = re.search(
                            r'(?:Local|LOCAL|Cidade|Endere√ßo)[:\s]+([A-Z√Å√â√ç√ì√ö√á][a-z√°√©√≠√≥√∫√ß]+(?:\s+[A-Z√Å√â√ç√ì√ö√á][a-z√°√©√≠√≥√∫√ß]+)*)\s*[-‚Äì]\s*([A-Z]{2})',
                            texto_completo,
                            re.IGNORECASE
                        )
                        if match:
                            return f"{match.group(1)} - {match.group(2)}"
                        
                        # Padr√£o alternativo: apenas cidade - estado
                        match = re.search(
                            r'([A-Z√Å√â√ç√ì√ö√á][a-z√°√©√≠√≥√∫√ß]+(?:\s+[A-Z√Å√â√ç√ì√ö√á][a-z√°√©√≠√≥√∫√ß]+)*)\s*[-‚Äì]\s*([A-Z]{2})',
                            texto_completo
                        )
                        if match:
                            cidade = match.group(1).strip()
                            estado = match.group(2).strip()
                            # Validar que n√£o √© uma data ou n√∫mero
                            if not re.search(r'\d{2}/\d{2}', cidade) and len(cidade) > 3:
                                return f"{cidade} - {estado}"
                        
                        # Tentar pegar texto ap√≥s os dois pontos
                        if ':' in texto_completo:
                            partes = texto_completo.split(':')
                            if len(partes) > 1:
                                local = partes[1].strip()
                                # Limpar local (remover datas, n√∫meros, etc)
                                local = re.sub(r'\d{2}/\d{2}/\d{4}.*$', '', local).strip()
                                local = re.sub(r'\s+', ' ', local)
                                if len(local) > 3 and len(local) < 100:
                                    return local
            
            # Estrat√©gia 3: Buscar em tabelas
            tabelas = soup.find_all('table')
            for tabela in tabelas:
                linhas = tabela.find_all('tr')
                for linha in linhas:
                    texto_linha = linha.get_text()
                    if re.search(r'local|cidade|endere√ßo', texto_linha, re.IGNORECASE):
                        tds = linha.find_all('td')
                        if len(tds) >= 2:
                            local = tds[1].get_text(strip=True)
                            if len(local) > 3 and len(local) < 100:
                                return local
        except Exception as e:
            self.logger.debug(f"Erro ao extrair local: {e}")
        return "N/A"
    
    def executar_scraping(self, categorias: List[str] = None, max_paginas: int = None):
        """Executa o scraping completo para quadros e esculturas"""
        self.logger.info("=== INICIANDO SCRAPING DO LEIL√ïESBR ===")
        self.logger.info(f"URL Base: {self.base_url}")
        self._parar_scraping = False
        
        # Se n√£o especificou categorias, buscar ambas
        if categorias is None:
            categorias = ["quadros", "esculturas"]
        
        self.logger.info(f"Categorias a coletar: {', '.join(categorias)}")
        
        total_obras_coletadas = 0
        
        # Processar cada categoria
        for categoria in categorias:
            if self._parar_scraping:
                break
            
            if categoria.lower() not in ["quadros", "esculturas"]:
                self.logger.warning(f"Categoria inv√°lida ignorada: {categoria}")
                continue
            
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"COLETANDO: {categoria.upper()}")
            self.logger.info(f"{'='*60}")
            
            # URL espec√≠fica para a categoria
            if categoria.lower() == "quadros":
                url_categoria = self.url_quadros
            else:
                url_categoria = self.url_esculturas
            
            # Descobrir total de p√°ginas
            if max_paginas is None:
                total_paginas = self.descobrir_total_paginas(categoria=categoria)
            else:
                total_paginas = max_paginas
                self.logger.info(f"Usando limite de {max_paginas} p√°ginas para {categoria}")
            
            if total_paginas == 0:
                self.logger.warning(f"Nenhuma p√°gina encontrada para {categoria}!")
                continue
            
            self.logger.info(f"Iniciando coleta de {total_paginas} p√°ginas de {categoria}...")
            
            # Processar cada p√°gina
            for pagina in range(1, total_paginas + 1):
                if self._parar_scraping:
                    self.logger.warning(f"‚ö†Ô∏è Scraping interrompido pelo usu√°rio na p√°gina {pagina} de {categoria}")
                    break
                
                # Construir URL da p√°gina
                if pagina == 1:
                    url = url_categoria
                else:
                    # Adicionar par√¢metro de p√°gina
                    # Para busca_andamento.asp, usar par√¢metro 'b'
                    # Para buscapos.asp, usar par√¢metro 'pagina'
                    if 'busca_andamento' in url_categoria:
                        # Substituir ou adicionar par√¢metro 'b'
                        if '&b=' in url_categoria:
                            url = re.sub(r'&b=\d+', f'&b={pagina - 1}', url_categoria)
                        else:
                            url = f"{url_categoria}&b={pagina - 1}"
                    else:
                        # Para buscapos.asp
                        if '?' in url_categoria:
                            url = f"{url_categoria}&pagina={pagina}"
                        else:
                            url = f"{url_categoria}?pagina={pagina}"
                
                self.processar_pagina(url, pagina, categoria=categoria)
                
                # Log de progresso
                if pagina % 5 == 0:
                    self.logger.info(f"üìà Progresso {categoria}: {pagina}/{total_paginas} p√°ginas | {len(self.dados_obras)} obras coletadas")
            
            obras_categoria = len(self.dados_obras) - total_obras_coletadas
            total_obras_coletadas = len(self.dados_obras)
            self.logger.info(f"‚úÖ {categoria.capitalize()}: {obras_categoria} obras coletadas")
        
        if self._parar_scraping:
            self.logger.info("=== SCRAPING INTERROMPIDO PELO USU√ÅRIO ===")
        else:
            self.logger.info("=== SCRAPING CONCLU√çDO ===")
        
        self.logger.info(f"Total de obras coletadas: {len(self.dados_obras)}")
        self.logger.info(f"Total de obras √∫nicas (sem duplicatas): {len(self.urls_coletadas)}")
